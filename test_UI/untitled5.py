# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t_iyCp4yZfwdHI4UOk3SsDjFYB0NbZV-
"""


# Activate 4-bit precision base model loading
use_4bit = True

# Compute dtype for 4-bit base models
bnb_4bit_compute_dtype = "float16"

# Quantization type (fp4 or nf4)
bnb_4bit_quant_type = "nf4"

# Activate nested quantization for 4-bit base models (double quantization)
use_nested_quant = False

import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer
from langchain.prompts import PromptTemplate
from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer,BitsAndBytesConfig
import torch


# Set the page configuration for Streamlit
st.set_page_config(page_title="Generate Blogs", page_icon='ðŸ¤–', layout='centered', initial_sidebar_state='collapsed')
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_storage=torch.bfloat16
)
#print(torch.cuda.current_device())
# Load your model and tokenizer
model_name = "rifatrzn/llama2-querymodel-v1"
model = AutoModelForCausalLM.from_pretrained(model_name,device_map={"": torch.cuda.current_device()},quantization_config=bnb_config)
tokenizer = AutoTokenizer.from_pretrained(model_name)


# Wrap the model and tokenizer in the langchain wrapper
pipe=pipeline("text-generation", model=model, tokenizer=tokenizer,max_new_tokens=256,temperature=0.01)
hf = HuggingFacePipeline(pipeline=pipe)

template = """{prompt}"""

prompt = PromptTemplate.from_template(template)

chain= prompt|hf
# Define the function to generate text based on the prompt
def getLLamaresponse(input_text):
    # Format the prompt using a template
    prompt = f"What is the best type of graph to visualize {input_text}?"
    # pipe(f"<s>[INST] {question} [/INST]")
    # Generate the response using the text generation pipeline
    # response = generator(prompt, max_new_tokens=256, temperature=0.01)
    generated_text=chain.invoke(prompt)
    print(generated_text)
    return generated_text



# UI setup
st.header("Generate Visualization Suggestions ðŸ¤–")

# Input from user
input_text = st.selectbox('Select a question:', [
    "distribution of plans by metal level",
    "average premiums by state",
    "plan availability by county",
    "Loan repayment by employment type"
])


# Button to generate prompt
if st.button("Generate Prompt"):
    with st.spinner('Generating...'):
        response = getLLamaresponse(input_text)
        st.markdown(f"**Prompt:** {input_text}\n\n**Response:**\n\n{response}")

